transferring data
Tue Jul  6 17:20:02 PDT 2021

Finished transferring
Tue Jul  6 17:20:03 PDT 2021

Adas: Argument Parser Options
---------------------------------------------
    config              : PostTrainingConfigs/BACH_transformed_testing/AdamP/lr-0.001-config-AdamP.yaml
    data                : /localscratch/zhan8425.7068030.0        
    output              : ImageNet_post_trained/BACH_transformed/AdamP/output/fine_tuning
    checkpoint          : ImageNet_post_trained/BACH_transformed/AdamP/checkpoint/fine_tuning/lr-0.001
    resume              : None                                    
    pretrained_model    : ImageNet                                
    freeze_encoder      : 1                                       
    root                : .                                       
    save_freq           : 200                                     
    cpu                 : 0                                       
    gpu                 : 0                                       
    mpd                 : 0                                       
    dist_url            : tcp://127.0.0.1:23456                   
    dist_backend        : nccl                                    
    world_size          : -1                                      
    rank                : -1                                      
---------------------------------------------
Adas: Output dir ImageNet_post_trained/BACH_transformed/AdamP/output/fine_tuning does not exist, building
Adas: Notice: early stop will not be used as it was set to -1.0, training till completion
Adas: Experiment Configuration
---------------------------------------------
    dataset              BACH_transformed    
    network              ResNet18            
    optimizer            AdamP               
    scheduler            None                
    level                L3Only              
    color_kwargs         {'augmentation': 'Color-Distortion', 'distortion': 0.3}
    degree_of_rotation   45                  
    vertical_flipping    0.5                 
    horizontal_flipping  0.5                 
    horizontal_shift     0.1                 
    vertical_shift       0.1                 
    gaussian_blur        0                   
    kernel_size          9                   
    variance             0.1                 
    cutout               0                   
    n_holes              1                   
    cutout_length        16                  
    init_lr              0.001               
    early_stop_threshold -1.0                
    optimizer_kwargs     {'weight_decay': 0.0005}
    scheduler_kwargs     {'gamma': 0.5, 'step_size': 20.0}
    p                    1                   
    start_trial          0                   
    n_trials             3                   
    num_workers          4                   
    max_epochs           200                 
    mini_batch_size      32                  
    loss                 cross_entropy       
    early_stop_patience  10                  
---------------------------------------------
Adas: Pytorch device is set to cuda
Traceback (most recent call last):
  File "src/adas/train.py", line 874, in <module>
    main(args)
  File "src/adas/train.py", line 837, in main
    main_worker(args.gpu, ngpus_per_node, args)
  File "src/adas/train.py", line 866, in main_worker
    training_agent.train()
  File "src/adas/train.py", line 381, in train
    self.reset(learning_rate)
  File "src/adas/train.py", line 350, in reset
    self.network = self.network.cuda(self.gpu)
  File "/project/6060173/zhan8425/HistoKT/ENV/lib/python3.8/site-packages/torch/nn/modules/module.py", line 491, in cuda
    return self._apply(lambda t: t.cuda(device))
  File "/project/6060173/zhan8425/HistoKT/ENV/lib/python3.8/site-packages/torch/nn/modules/module.py", line 387, in _apply
    module._apply(fn)
  File "/project/6060173/zhan8425/HistoKT/ENV/lib/python3.8/site-packages/torch/nn/modules/module.py", line 409, in _apply
    param_applied = fn(param)
  File "/project/6060173/zhan8425/HistoKT/ENV/lib/python3.8/site-packages/torch/nn/modules/module.py", line 491, in <lambda>
    return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
Adas: Argument Parser Options
---------------------------------------------
    config              : PostTrainingConfigs/BACH_transformed_testing/AdamP/lr-0.001-config-AdamP.yaml
    data                : /localscratch/zhan8425.7068030.0        
    output              : ImageNet_post_trained/BACH_transformed/AdamP/output/deep_tuning
    checkpoint          : ImageNet_post_trained/BACH_transformed/AdamP/checkpoint/deep_tuning/lr-0.001
    resume              : None                                    
    pretrained_model    : ImageNet                                
    freeze_encoder      : 0                                       
    root                : .                                       
    save_freq           : 200                                     
    cpu                 : 0                                       
    gpu                 : 0                                       
    mpd                 : 0                                       
    dist_url            : tcp://127.0.0.1:23456                   
    dist_backend        : nccl                                    
    world_size          : -1                                      
    rank                : -1                                      
---------------------------------------------
Adas: Output dir ImageNet_post_trained/BACH_transformed/AdamP/output/deep_tuning does not exist, building
Adas: Notice: early stop will not be used as it was set to -1.0, training till completion
Adas: Experiment Configuration
---------------------------------------------
    dataset              BACH_transformed    
    network              ResNet18            
    optimizer            AdamP               
    scheduler            None                
    level                L3Only              
    color_kwargs         {'augmentation': 'Color-Distortion', 'distortion': 0.3}
    degree_of_rotation   45                  
    vertical_flipping    0.5                 
    horizontal_flipping  0.5                 
    horizontal_shift     0.1                 
    vertical_shift       0.1                 
    gaussian_blur        0                   
    kernel_size          9                   
    variance             0.1                 
    cutout               0                   
    n_holes              1                   
    cutout_length        16                  
    init_lr              0.001               
    early_stop_threshold -1.0                
    optimizer_kwargs     {'weight_decay': 0.0005}
    scheduler_kwargs     {'gamma': 0.5, 'step_size': 20.0}
    p                    1                   
    start_trial          0                   
    n_trials             3                   
    num_workers          4                   
    max_epochs           200                 
    mini_batch_size      32                  
    loss                 cross_entropy       
    early_stop_patience  10                  
---------------------------------------------
Adas: Pytorch device is set to cuda
Traceback (most recent call last):
  File "src/adas/train.py", line 874, in <module>
    main(args)
  File "src/adas/train.py", line 837, in main
    main_worker(args.gpu, ngpus_per_node, args)
  File "src/adas/train.py", line 866, in main_worker
    training_agent.train()
  File "src/adas/train.py", line 381, in train
    self.reset(learning_rate)
  File "src/adas/train.py", line 350, in reset
    self.network = self.network.cuda(self.gpu)
  File "/project/6060173/zhan8425/HistoKT/ENV/lib/python3.8/site-packages/torch/nn/modules/module.py", line 491, in cuda
    return self._apply(lambda t: t.cuda(device))
  File "/project/6060173/zhan8425/HistoKT/ENV/lib/python3.8/site-packages/torch/nn/modules/module.py", line 387, in _apply
    module._apply(fn)
  File "/project/6060173/zhan8425/HistoKT/ENV/lib/python3.8/site-packages/torch/nn/modules/module.py", line 409, in _apply
    param_applied = fn(param)
  File "/project/6060173/zhan8425/HistoKT/ENV/lib/python3.8/site-packages/torch/nn/modules/module.py", line 491, in <lambda>
    return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory

transferring data
Mon Jul  5 10:07:20 PDT 2021

Finished transferring
Mon Jul  5 10:07:22 PDT 2021

Adas: Argument Parser Options
---------------------------------------------
    config              : PostTrainingConfigs/MHIST_transformed_testing/AdamP/lr-0.0005-config-AdamP.yaml
    data                : /localscratch/zhan8425.6997263.0        
    output              : ADP_post_trained/MHIST_transformed/AdamP/output/fine_tuning
    checkpoint          : ADP_post_trained/MHIST_transformed/AdamP/checkpoint/fine_tuning/lr-0.0005
    resume              : None                                    
    pretrained_model    : /home/zhan8425/projects/def-plato/zhan8425/HistoKT/.Adas-checkpoint/ADP/best_trial_2.pth.tar
    freeze_encoder      : 1                                       
    root                : .                                       
    save_freq           : 200                                     
    cpu                 : 0                                       
    gpu                 : 0                                       
    mpd                 : 0                                       
    dist_url            : tcp://127.0.0.1:23456                   
    dist_backend        : nccl                                    
    world_size          : -1                                      
    rank                : -1                                      
---------------------------------------------
Adas: Notice: early stop will not be used as it was set to -1.0, training till completion
Adas: Experiment Configuration
---------------------------------------------
    dataset              MHIST_transformed   
    network              ResNet18            
    optimizer            AdamP               
    scheduler            None                
    level                L3Only              
    color_kwargs         {'augmentation': 'Color-Distortion', 'distortion': 0.3}
    degree_of_rotation   45                  
    vertical_flipping    0.5                 
    horizontal_flipping  0.5                 
    horizontal_shift     0.1                 
    vertical_shift       0.1                 
    gaussian_blur        0                   
    kernel_size          9                   
    variance             0.1                 
    cutout               0                   
    n_holes              1                   
    cutout_length        16                  
    init_lr              0.0005              
    early_stop_threshold -1.0                
    optimizer_kwargs     {'weight_decay': 0.0005}
    scheduler_kwargs     {'gamma': 0.5, 'step_size': 20.0}
    p                    1                   
    start_trial          0                   
    n_trials             3                   
    num_workers          4                   
    max_epochs           200                 
    mini_batch_size      32                  
    loss                 cross_entropy       
    early_stop_patience  10                  
---------------------------------------------
Adas: Pytorch device is set to cuda
Traceback (most recent call last):
  File "src/adas/train.py", line 871, in <module>
    main(args)
  File "src/adas/train.py", line 834, in main
    main_worker(args.gpu, ngpus_per_node, args)
  File "src/adas/train.py", line 863, in main_worker
    training_agent.train()
  File "src/adas/train.py", line 378, in train
    self.reset(learning_rate)
  File "src/adas/train.py", line 347, in reset
    self.network = self.network.cuda(self.gpu)
  File "/project/6060173/zhan8425/HistoKT/ENV/lib/python3.8/site-packages/torch/nn/modules/module.py", line 491, in cuda
    return self._apply(lambda t: t.cuda(device))
  File "/project/6060173/zhan8425/HistoKT/ENV/lib/python3.8/site-packages/torch/nn/modules/module.py", line 387, in _apply
    module._apply(fn)
  File "/project/6060173/zhan8425/HistoKT/ENV/lib/python3.8/site-packages/torch/nn/modules/module.py", line 409, in _apply
    param_applied = fn(param)
  File "/project/6060173/zhan8425/HistoKT/ENV/lib/python3.8/site-packages/torch/nn/modules/module.py", line 491, in <lambda>
    return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
Adas: Argument Parser Options
---------------------------------------------
    config              : PostTrainingConfigs/MHIST_transformed_testing/AdamP/lr-0.0005-config-AdamP.yaml
    data                : /localscratch/zhan8425.6997263.0        
    output              : ADP_post_trained/MHIST_transformed/AdamP/output/deep_tuning
    checkpoint          : ADP_post_trained/MHIST_transformed/AdamP/checkpoint/deep_tuning/lr-0.0005
    resume              : None                                    
    pretrained_model    : /home/zhan8425/projects/def-plato/zhan8425/HistoKT/.Adas-checkpoint/ADP/best_trial_2.pth.tar
    freeze_encoder      : 0                                       
    root                : .                                       
    save_freq           : 200                                     
    cpu                 : 0                                       
    gpu                 : 0                                       
    mpd                 : 0                                       
    dist_url            : tcp://127.0.0.1:23456                   
    dist_backend        : nccl                                    
    world_size          : -1                                      
    rank                : -1                                      
---------------------------------------------
Adas: Notice: early stop will not be used as it was set to -1.0, training till completion
Adas: Experiment Configuration
---------------------------------------------
    dataset              MHIST_transformed   
    network              ResNet18            
    optimizer            AdamP               
    scheduler            None                
    level                L3Only              
    color_kwargs         {'augmentation': 'Color-Distortion', 'distortion': 0.3}
    degree_of_rotation   45                  
    vertical_flipping    0.5                 
    horizontal_flipping  0.5                 
    horizontal_shift     0.1                 
    vertical_shift       0.1                 
    gaussian_blur        0                   
    kernel_size          9                   
    variance             0.1                 
    cutout               0                   
    n_holes              1                   
    cutout_length        16                  
    init_lr              0.0005              
    early_stop_threshold -1.0                
    optimizer_kwargs     {'weight_decay': 0.0005}
    scheduler_kwargs     {'gamma': 0.5, 'step_size': 20.0}
    p                    1                   
    start_trial          0                   
    n_trials             3                   
    num_workers          4                   
    max_epochs           200                 
    mini_batch_size      32                  
    loss                 cross_entropy       
    early_stop_patience  10                  
---------------------------------------------
Adas: Pytorch device is set to cuda
Traceback (most recent call last):
  File "src/adas/train.py", line 871, in <module>
    main(args)
  File "src/adas/train.py", line 834, in main
    main_worker(args.gpu, ngpus_per_node, args)
  File "src/adas/train.py", line 863, in main_worker
    training_agent.train()
  File "src/adas/train.py", line 378, in train
    self.reset(learning_rate)
  File "src/adas/train.py", line 347, in reset
    self.network = self.network.cuda(self.gpu)
  File "/project/6060173/zhan8425/HistoKT/ENV/lib/python3.8/site-packages/torch/nn/modules/module.py", line 491, in cuda
    return self._apply(lambda t: t.cuda(device))
  File "/project/6060173/zhan8425/HistoKT/ENV/lib/python3.8/site-packages/torch/nn/modules/module.py", line 387, in _apply
    module._apply(fn)
  File "/project/6060173/zhan8425/HistoKT/ENV/lib/python3.8/site-packages/torch/nn/modules/module.py", line 409, in _apply
    param_applied = fn(param)
  File "/project/6060173/zhan8425/HistoKT/ENV/lib/python3.8/site-packages/torch/nn/modules/module.py", line 491, in <lambda>
    return self._apply(lambda t: t.cuda(device))
RuntimeError: CUDA error: out of memory
